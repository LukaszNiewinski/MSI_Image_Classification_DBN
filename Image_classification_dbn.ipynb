{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Images classification using Deep Belief Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "\n",
    "from dbn.tensorflow import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read keys stored within an annots dict:\n",
      "\n",
      " dict_keys(['__header__', '__version__', '__globals__', 'classnames', 'train_data', 'val_data', 'test_data', 'train_labels', 'val_labels', 'test_labels']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data from matlab .mat file\n",
    "data_dir = 'data'\n",
    "# dir for data files 16px and 28 pix\n",
    "data_dir_16split = data_dir + '/caltech101_silhouettes_16_split1.mat'\n",
    "data_dir_28split = data_dir + '/caltech101_silhouettes_28_split1.mat'\n",
    "\n",
    "# define range of classes, between 1-101  \n",
    "left = 1\n",
    "right = 33\n",
    "\n",
    "# data reads from .mat file are stored within dict list annots\n",
    "annots = loadmat(data_dir_28split)\n",
    "\n",
    "# Create list of all classes read from data file \n",
    "classnames = []\n",
    "class_num_to_name = dict()\n",
    "# a mapping dictionary from class number to its name\n",
    "for i in range(left-1, right):\n",
    "    classnames.append(annots['classnames'][0][i][0])\n",
    "    class_num_to_name[i+1] = annots['classnames'][0][i][0]\n",
    "    \n",
    "print(\"Read keys stored within an annots dict:\\n\\n\", annots.keys(), \"\\n\")\n",
    "\n",
    "# print(\"A list of\", left,\"-\", right, \"mapped classnames:\\n\")\n",
    "# print(class_num_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function dat extract data with labels within range <left, right>\n",
    "def get_proper_classes(data_name, label_name, left, right):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    i = 0\n",
    "    for img in annots[data_name]:\n",
    "        if annots[label_name][i] <= right and annots[label_name][i] >= left:\n",
    "            data.append(img)\n",
    "            labels.append(annots[label_name][i])\n",
    "        i += 1 \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels).flatten()\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (1587, 784) train_labels shape: (1587,)\n",
      "val_data shape: (1428, 784) val_labels shape: (1428,)\n",
      "test_data shape: (1441, 784) test_labels shape: (1441,)\n"
     ]
    }
   ],
   "source": [
    "# an image is represented by an array of 256 or 784 elements (16x16pix or 28x28pix)\n",
    "train_data, train_labels = get_proper_classes('train_data', 'train_labels', left, right)\n",
    "val_data, val_labels = get_proper_classes('val_data', 'val_labels', left, right)\n",
    "test_data, test_labels = get_proper_classes('test_data', 'test_labels', left, right)\n",
    "\n",
    "# simple check if each image has it's label\n",
    "print(\"train_data shape:\", train_data.shape, \"train_labels shape:\", train_labels.shape)\n",
    "print(\"val_data shape:\", val_data.shape, \"val_labels shape:\", val_labels.shape)\n",
    "print(\"test_data shape:\", test_data.shape, \"test_labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that counts quantity of images of each class\n",
    "def count_images(dictionary, label_name, label):\n",
    "    for x in label:\n",
    "        dic_counts[label_name][x] = dic_counts[label_name].get(x) + 1 \n",
    "    return dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCLASS NAME          ID         TRAIN      VAL        TEST       \u001b[0m\n",
      "Airplanes Side 2    1          100        349        349       \n",
      "Faces 2             2          100        167        168       \n",
      "Faces 3             3          100        167        168       \n",
      "Leopards            4          100        50         50        \n",
      "Motorbikes 16       5          100        349        349       \n",
      "accordion           6          33         11         11        \n",
      "anchor              7          26         8          8         \n",
      "ant                 8          26         8          8         \n",
      "barrel              9          29         9          9         \n",
      "bass                10         33         10         11        \n",
      "beaver              11         28         9          9         \n",
      "binocular           12         20         6          7         \n",
      "bonsai              13         77         25         26        \n",
      "brain               14         59         19         20        \n",
      "brontosaurus        15         26         8          9         \n",
      "buddha              16         51         17         17        \n",
      "butterfly           17         55         18         18        \n",
      "camera              18         30         9          10        \n",
      "cannon              19         26         8          9         \n",
      "car side            20         74         24         25        \n",
      "ceiling fan         21         29         9          9         \n",
      "cellphone           22         36         11         12        \n",
      "chair               23         38         12         12        \n",
      "chandelier          24         65         21         21        \n",
      "cougar body         25         29         9          9         \n",
      "cougar face         26         42         13         14        \n",
      "crab                27         44         14         15        \n",
      "crayfish            28         42         14         14        \n",
      "crocodile           29         30         10         10        \n",
      "crocodile head      30         31         10         10        \n",
      "cup                 31         35         11         11        \n",
      "dalmatian           32         41         13         13        \n",
      "dollar bill         33         32         10         10        \n",
      "\n",
      "total number of elements: 4456\n"
     ]
    }
   ],
   "source": [
    "# count number of images of each class within each label group\n",
    "labels_names = ['train_labels', 'val_labels', 'test_labels']\n",
    "\n",
    "# classes are within range\n",
    "classes = [i for i in range(left, right+1)]\n",
    "\n",
    "# create empty nested dictionaries to store quantity of each class img within each group\n",
    "dic_counts = { label:{ class_id: 0 for class_id in classes} for label in labels}\n",
    "\n",
    "# count number of occurences of each class within each group\n",
    "dic_counts = count_images(dic_counts, 'train_labels', train_labels)\n",
    "dic_counts = count_images(dic_counts, 'val_labels', val_labels)\n",
    "dic_counts = count_images(dic_counts, 'test_labels', test_labels)\n",
    "        \n",
    "# print data\n",
    "total =0\n",
    "print(\"\\033[1m{:<20}{:<10} {:<10} {:<10} {:<10} \\033[0m\".format('CLASS NAME', 'ID', 'TRAIN', \"VAL\", \"TEST\"))\n",
    "for class_id in classes:\n",
    "    print('{:<20}{:<10} {:<10} {:<10} {:<10}'.format(classnames[class_id-1], class_id, dic_counts['train_labels'][class_id],\n",
    "                                                 dic_counts['val_labels'][class_id],\n",
    "                                                 dic_counts['test_labels'][class_id]))\n",
    "#     # restrict output rows to 10 elements\n",
    "#     if class_id == 10:\n",
    "#         print('{:<20}{:<10} {:<10} {:<10} {:<10}'.format(\"...\", \"...\", \"...\", \"...\", \"...\"))\n",
    "#         break\n",
    "total = train_data.shape[0] + val_data.shape[0] + test_data.shape[0]\n",
    "print(\"\\ntotal number of elements:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of digit images of size: 28 x 28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  brain                      Motorbikes 16            Airplanes Side 2          Leopards                  brontosaurus             \n"
     ]
    }
   ],
   "source": [
    "# displaying an exmaple images from a ndarray format\n",
    "# set width and height, depends on read data\n",
    "if annots['train_data'].shape[1] == 256:\n",
    "    w, h = 16, 16\n",
    "else:\n",
    "    w, h = 28, 28\n",
    "\n",
    "print(\"Example of digit images of size:\", w, \"x\", h )\n",
    "# an id's of 5 example images to be displayed, possible range: 'train_data' 0 - 4081, 'val_data' 0 - 2256, 'test_data' 0 - 2301 \n",
    "# generate 5 random integers within proper range for chosen label\n",
    "ids = [random.randint(0,train_data.shape[0]) for _ in range(5)]\n",
    "images = []\n",
    "for id in ids:\n",
    "    image = train_data[id]\n",
    "    # ndarray to store data that can be displayed, width x height x RGB\n",
    "    data = np.zeros((h, w, 3), dtype = np.uint8)\n",
    "    row = 0\n",
    "    column = 0 \n",
    "    for x in image:\n",
    "        if x == 1:\n",
    "            data[row, column]=[255, 255, 255]\n",
    "        row += 1 \n",
    "        if row == h:\n",
    "            column += 1 \n",
    "            row = 0\n",
    "    images.append(data)\n",
    "# matplot used to display an image\n",
    "i = 1\n",
    "rows = 1\n",
    "columns = len(ids)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for data in images:\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    i += 1 \n",
    "plt.show()\n",
    "class_examples = [classnames[train_labels[x]-1] for x in ids]\n",
    "print('  {:<27}{:<25}{:<26}{:<26}{:<25}'.format(class_examples[0], class_examples[1], class_examples[2], class_examples[3], class_examples[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare structure store the results\n",
    "Result = namedtuple('Result', 'h_structure, n_epochs batch_size learning_rate accuracy')\n",
    "results = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (3015, 784)\n",
      "train labels size (3015,)\n",
      "test data size: (1441, 784)\n",
      "test labels size: (1441,)\n"
     ]
    }
   ],
   "source": [
    "# prepare input data\n",
    "train_val = np.concatenate((train_data, val_data), axis=0).astype(np.float32)\n",
    "labels_train_val = np.concatenate((train_labels, val_labels), axis=0)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(train, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"train data size:\", train_val.shape) \n",
    "print(\"train labels size\", labels_train_val.shape)\n",
    "print(\"test data size:\", test_data.shape)\n",
    "print(\"test labels size:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters:\n",
    "hidden_layer_structure = [128,128]\n",
    "n_epochs_rbm = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 424928.937500\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 322314.343750\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 461072.468750\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 401535.781250\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 444005.375000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 394695.718750\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 372172.625000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 433543.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 484111.906250\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 600452.687500\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 81356193792.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 81992835072.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 73026723840.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 60178399232.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 71381319680.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 68422905856.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 69932711936.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 66765635584.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 69183594496.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 62971514880.000000\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.880200\n",
      ">> Epoch 1 finished \tANN training loss 2.709118\n",
      ">> Epoch 2 finished \tANN training loss 2.825894\n",
      ">> Epoch 3 finished \tANN training loss 2.658984\n",
      ">> Epoch 4 finished \tANN training loss 2.552594\n",
      ">> Epoch 5 finished \tANN training loss 2.570407\n",
      ">> Epoch 6 finished \tANN training loss 2.462087\n",
      ">> Epoch 7 finished \tANN training loss 2.429033\n",
      ">> Epoch 8 finished \tANN training loss 2.215017\n",
      ">> Epoch 9 finished \tANN training loss 2.047543\n",
      ">> Epoch 10 finished \tANN training loss 1.945431\n",
      ">> Epoch 11 finished \tANN training loss 1.944017\n",
      ">> Epoch 12 finished \tANN training loss 1.916388\n",
      ">> Epoch 13 finished \tANN training loss 2.168930\n",
      ">> Epoch 14 finished \tANN training loss 1.965735\n",
      ">> Epoch 15 finished \tANN training loss 1.861064\n",
      ">> Epoch 16 finished \tANN training loss 1.990246\n",
      ">> Epoch 17 finished \tANN training loss 1.869838\n",
      ">> Epoch 18 finished \tANN training loss 1.917323\n",
      ">> Epoch 19 finished \tANN training loss 1.884886\n",
      ">> Epoch 20 finished \tANN training loss 1.805165\n",
      ">> Epoch 21 finished \tANN training loss 1.774230\n",
      ">> Epoch 22 finished \tANN training loss 1.883820\n",
      ">> Epoch 23 finished \tANN training loss 1.694578\n",
      ">> Epoch 24 finished \tANN training loss 1.751628\n",
      ">> Epoch 25 finished \tANN training loss 1.800067\n",
      ">> Epoch 26 finished \tANN training loss 1.728369\n",
      ">> Epoch 27 finished \tANN training loss 1.782163\n",
      ">> Epoch 28 finished \tANN training loss 2.001263\n",
      ">> Epoch 29 finished \tANN training loss 1.836729\n",
      ">> Epoch 30 finished \tANN training loss 1.746678\n",
      ">> Epoch 31 finished \tANN training loss 1.742385\n",
      ">> Epoch 32 finished \tANN training loss 2.470260\n",
      ">> Epoch 33 finished \tANN training loss 1.705582\n",
      ">> Epoch 34 finished \tANN training loss 1.706446\n",
      ">> Epoch 35 finished \tANN training loss 2.130766\n",
      ">> Epoch 36 finished \tANN training loss 1.693194\n",
      ">> Epoch 37 finished \tANN training loss 1.950310\n",
      ">> Epoch 38 finished \tANN training loss 3.105696\n",
      ">> Epoch 39 finished \tANN training loss 1.831673\n",
      ">> Epoch 40 finished \tANN training loss 1.642507\n",
      ">> Epoch 41 finished \tANN training loss 1.789607\n",
      ">> Epoch 42 finished \tANN training loss 1.618638\n",
      ">> Epoch 43 finished \tANN training loss 1.683935\n",
      ">> Epoch 44 finished \tANN training loss 4.388330\n",
      ">> Epoch 45 finished \tANN training loss 2.078299\n",
      ">> Epoch 46 finished \tANN training loss 1.579930\n",
      ">> Epoch 47 finished \tANN training loss 1.639625\n",
      ">> Epoch 48 finished \tANN training loss 1.691365\n",
      ">> Epoch 49 finished \tANN training loss 1.740789\n",
      ">> Epoch 50 finished \tANN training loss 1.677957\n",
      ">> Epoch 51 finished \tANN training loss 1.684811\n",
      ">> Epoch 52 finished \tANN training loss 1.569084\n",
      ">> Epoch 53 finished \tANN training loss 1.591101\n",
      ">> Epoch 54 finished \tANN training loss 1.731220\n",
      ">> Epoch 55 finished \tANN training loss 1.868236\n",
      ">> Epoch 56 finished \tANN training loss 1.572957\n",
      ">> Epoch 57 finished \tANN training loss 1.624382\n",
      ">> Epoch 58 finished \tANN training loss 1.887211\n",
      ">> Epoch 59 finished \tANN training loss 1.684935\n",
      ">> Epoch 60 finished \tANN training loss 2.082108\n",
      ">> Epoch 61 finished \tANN training loss 1.601726\n",
      ">> Epoch 62 finished \tANN training loss 1.645129\n",
      ">> Epoch 63 finished \tANN training loss 1.704175\n",
      ">> Epoch 64 finished \tANN training loss 1.729308\n",
      ">> Epoch 65 finished \tANN training loss 1.862170\n",
      ">> Epoch 66 finished \tANN training loss 1.637558\n",
      ">> Epoch 67 finished \tANN training loss 1.744231\n",
      ">> Epoch 68 finished \tANN training loss 1.756609\n",
      ">> Epoch 69 finished \tANN training loss 1.545325\n",
      ">> Epoch 70 finished \tANN training loss 1.598040\n",
      ">> Epoch 71 finished \tANN training loss 1.954240\n",
      ">> Epoch 72 finished \tANN training loss 2.000465\n",
      ">> Epoch 73 finished \tANN training loss 1.746532\n",
      ">> Epoch 74 finished \tANN training loss 1.581669\n",
      ">> Epoch 75 finished \tANN training loss 1.540892\n",
      ">> Epoch 76 finished \tANN training loss 1.769879\n",
      ">> Epoch 77 finished \tANN training loss 1.808052\n",
      ">> Epoch 78 finished \tANN training loss 2.186451\n",
      ">> Epoch 79 finished \tANN training loss 1.849041\n",
      ">> Epoch 80 finished \tANN training loss 1.529963\n",
      ">> Epoch 81 finished \tANN training loss 1.561936\n",
      ">> Epoch 82 finished \tANN training loss 1.667686\n",
      ">> Epoch 83 finished \tANN training loss 1.682915\n",
      ">> Epoch 84 finished \tANN training loss 1.440030\n",
      ">> Epoch 85 finished \tANN training loss 1.467528\n",
      ">> Epoch 86 finished \tANN training loss 3.153295\n",
      ">> Epoch 87 finished \tANN training loss 1.532822\n",
      ">> Epoch 88 finished \tANN training loss 1.904783\n",
      ">> Epoch 89 finished \tANN training loss 1.622409\n",
      ">> Epoch 90 finished \tANN training loss 1.735470\n",
      ">> Epoch 91 finished \tANN training loss 1.821939\n",
      ">> Epoch 92 finished \tANN training loss 1.499498\n",
      ">> Epoch 93 finished \tANN training loss 1.480610\n",
      ">> Epoch 94 finished \tANN training loss 1.668756\n",
      ">> Epoch 95 finished \tANN training loss 1.572984\n",
      ">> Epoch 96 finished \tANN training loss 1.460420\n",
      ">> Epoch 97 finished \tANN training loss 1.561207\n",
      ">> Epoch 98 finished \tANN training loss 1.527297\n",
      ">> Epoch 99 finished \tANN training loss 1.739615\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.563498\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "classifier = SupervisedDBNClassification(hidden_layers_structure = hidden_layer_structure,\n",
    "                                         learning_rate_rbm = learning_rate,\n",
    "                                         learning_rate = 0.1,\n",
    "                                         n_epochs_rbm = n_epochs_rbm,\n",
    "                                         n_iter_backprop = 100,\n",
    "                                         batch_size = batch_size,\n",
    "                                         activation_function = 'relu',\n",
    "                                         dropout_p = 0.2)\n",
    "#classifier.fit(train_data, train_labels)\n",
    "classifier.fit(train, labels)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_pred = classifier.predict(test_data)\n",
    "accuracy = accuracy_score(test_labels, test_pred)\n",
    "print('Done.\\nAccuracy: %f' % accuracy)\n",
    "result = Result(hidden_layer_structure, n_epochs_rbm, batch_size, learning_rate, accuracy)\n",
    "results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure [256, 256] epochs 10 batch size: 32 learning rate: 0.1 accuracy: 0.7265787647467037\n",
      "structure [256, 256] epochs 10 batch size: 64 learning rate: 0.1 accuracy: 0.24219292158223457\n",
      "structure [256, 256] epochs 10 batch size: 48 learning rate: 0.1 accuracy: 0.6863289382373352\n",
      "structure [256, 256] epochs 10 batch size: 16 learning rate: 0.1 accuracy: 0.6884108258154059\n",
      "structure [784, 784] epochs 10 batch size: 32 learning rate: 0.1 accuracy: 0.732824427480916\n",
      "structure [512, 512] epochs 10 batch size: 32 learning rate: 0.1 accuracy: 0.6835530881332408\n",
      "structure [128, 128] epochs 10 batch size: 32 learning rate: 0.1 accuracy: 0.563497571131159\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(\"structure\", result.h_structure, \"epochs\", result.n_epochs, \"batch size:\", result.batch_size, \"learning rate:\", result.learning_rate, \"accuracy:\", result.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
